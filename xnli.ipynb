{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b899c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample #0 (train) ===\n",
      "Goal: When boiling butter, when it's ready, you can\n",
      "Choice 0: Pour it onto a plate\n",
      "Choice 1: Pour it into a jar\n",
      "Answer: Choice 1\n",
      "\n",
      "--- Choice 0 ---\n",
      "entailment: 0.0048\n",
      "neutral: 0.9884\n",
      "contradiction: 0.0068\n",
      "Prediction: neutral\n",
      "Entailment - Contradiction score: -0.0020\n",
      "\n",
      "--- Choice 1 ---\n",
      "entailment: 0.0035\n",
      "neutral: 0.9784\n",
      "contradiction: 0.0181\n",
      "Prediction: neutral\n",
      "Entailment - Contradiction score: -0.0146\n",
      "\n",
      "Recommended Choice: 0 (score -0.0020)\n",
      "\n",
      "=== Sample #4 (validation) ===\n",
      "Goal: ice box\n",
      "Choice 0: will turn into a cooler if you add water to it\n",
      "Choice 1: will turn into a cooler if you add soda to it\n",
      "Answer: Choice 0\n",
      "\n",
      "--- Choice 0 ---\n",
      "entailment: 0.0023\n",
      "neutral: 0.9925\n",
      "contradiction: 0.0052\n",
      "Prediction: neutral\n",
      "Entailment - Contradiction score: -0.0029\n",
      "\n",
      "--- Choice 1 ---\n",
      "entailment: 0.0027\n",
      "neutral: 0.9896\n",
      "contradiction: 0.0077\n",
      "Prediction: neutral\n",
      "Entailment - Contradiction score: -0.0050\n",
      "\n",
      "Recommended Choice: 0 (score -0.0029)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# 1. 데이터셋 불러오기\n",
    "# =========================\n",
    "dataset = load_dataset(\"piqa\", trust_remote_code=True)\n",
    "\n",
    "# =========================\n",
    "# 2. 모델과 토크나이저 불러오기\n",
    "# =========================\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "labels = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "\n",
    "# =========================\n",
    "# 3. PIQA 샘플 평가 함수 (방법2 적용)\n",
    "# =========================\n",
    "def evaluate_piqa_sample(index: int, split=\"train\"):\n",
    "    \"\"\"주어진 index의 PIQA 샘플을 NLI 모델로 평가하고 최종 선택지 추천\"\"\"\n",
    "    sample = dataset[split][index]\n",
    "    \n",
    "    premise = sample['goal']\n",
    "    choices = [sample['sol1'], sample['sol2']]\n",
    "    answer = sample['label']  # 0 또는 1\n",
    "\n",
    "    print(f\"=== Sample #{index} ({split}) ===\")\n",
    "    print(f\"Goal: {premise}\")\n",
    "    print(f\"Choice 0: {choices[0]}\")\n",
    "    print(f\"Choice 1: {choices[1]}\")\n",
    "    print(f\"Answer: Choice {answer}\\n\")\n",
    "\n",
    "    entailment_minus_contradiction = []\n",
    "\n",
    "    for i, choice in enumerate(choices):\n",
    "        # 토크나이징\n",
    "        inputs = tokenizer(premise, choice, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "        # 모델 예측\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # entailment - contradiction 점수 계산\n",
    "        score = probs[0][0].item() - probs[0][2].item()\n",
    "        entailment_minus_contradiction.append(score)\n",
    "\n",
    "        # 결과 출력\n",
    "        print(f\"--- Choice {i} ---\")\n",
    "        for label, prob in zip(labels, probs[0]):\n",
    "            print(f\"{label}: {prob:.4f}\")\n",
    "        pred_label = labels[probs.argmax()]\n",
    "        print(f\"Prediction: {pred_label}\")\n",
    "        print(f\"Entailment - Contradiction score: {score:.4f}\\n\")\n",
    "\n",
    "    # 최종 추천 선택지\n",
    "    final_choice = entailment_minus_contradiction.index(max(entailment_minus_contradiction))\n",
    "    print(f\"Recommended Choice: {final_choice} (score {max(entailment_minus_contradiction):.4f})\\n\")\n",
    "\n",
    "# =========================\n",
    "# 4. 사용 예시\n",
    "# =========================\n",
    "evaluate_piqa_sample(0)   # train셋 2번 샘플 평가\n",
    "evaluate_piqa_sample(4, split=\"validation\")  # validation셋 5번 샘플 평가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032772b5",
   "metadata": {},
   "source": [
    "## Instructured LLM + Chain-of-Reasoning + pairwise 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36559963",
   "metadata": {},
   "source": [
    "### flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc4e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample #2 (train) ===\n",
      "Goal: how do you indent something?\n",
      "Choice 0: leave a space before starting the writing\n",
      "Choice 1: press the spacebar\n",
      "Answer: Choice 0\n",
      "\n",
      "--- Choice 0 ---\n",
      "Reasoning: \n",
      "Score: 0.0\n",
      "\n",
      "--- Choice 1 ---\n",
      "Reasoning: \n",
      "Score: 0.0\n",
      "\n",
      "Recommended Choice: 0 (score 0.0000)\n",
      "\n",
      "=== Sample #5 (validation) ===\n",
      "Goal: Remove soap scum from shower door.\n",
      "Choice 0: Rub hard with bed sheets, then rinse.\n",
      "Choice 1: Rub hard with dryer sheets, then rinse.\n",
      "Answer: Choice 1\n",
      "\n",
      "--- Choice 0 ---\n",
      "Reasoning: \n",
      "Score: 0.0\n",
      "\n",
      "--- Choice 1 ---\n",
      "Reasoning: \n",
      "Score: 0.0\n",
      "\n",
      "Recommended Choice: 0 (score 0.0000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import json, re\n",
    "\n",
    "# =========================\n",
    "# 1. PIQA 데이터셋 불러오기\n",
    "# =========================\n",
    "dataset = load_dataset(\"piqa\", trust_remote_code=True)\n",
    "\n",
    "# =========================\n",
    "# 2. Instruction-tuned LLM 불러오기\n",
    "# =========================\n",
    "model_name = \"google/flan-t5-base\"  # 예시, 다른 instruction LLM 가능\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# =========================\n",
    "# 3. 평가 함수 (CoT + pairwise 점수화)\n",
    "# =========================\n",
    "def evaluate_piqa_sample_llm(index: int, split=\"train\", max_new_tokens=200):\n",
    "    sample = dataset[split][index]\n",
    "    goal = sample['goal']\n",
    "    choices = [sample['sol1'], sample['sol2']]\n",
    "    answer = sample['label']\n",
    "\n",
    "    print(f\"=== Sample #{index} ({split}) ===\")\n",
    "    print(f\"Goal: {goal}\")\n",
    "    print(f\"Choice 0: {choices[0]}\")\n",
    "    print(f\"Choice 1: {choices[1]}\")\n",
    "    print(f\"Answer: Choice {answer}\\n\")\n",
    "\n",
    "    # Prompt 생성\n",
    "    prompt = f\"\"\"\n",
    "Goal: {goal}\n",
    "\n",
    "Choice 0: {choices[0]}\n",
    "Choice 1: {choices[1]}\n",
    "\n",
    "For each choice:\n",
    "1. Explain step-by-step reasoning why this choice may or may not achieve the goal.\n",
    "2. Give a score from 0 to 1 for effectiveness.\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\n",
    " \"Choice 0\": {{\"reasoning\": \"...\", \"score\": 0.0}},\n",
    " \"Choice 1\": {{\"reasoning\": \"...\", \"score\": 0.0}}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    # Tokenize + Generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # JSON 파싱 및 점수 추출 (robust)\n",
    "    try:\n",
    "        output_json = json.loads(output_text)\n",
    "    except:\n",
    "        output_json = output_text  # JSON 파싱 실패 시 문자열 그대로\n",
    "\n",
    "    scores = []\n",
    "    for i in range(2):\n",
    "        # case 1: output_json이 dict일 경우\n",
    "        if isinstance(output_json, dict) and f\"Choice {i}\" in output_json:\n",
    "            entry = output_json[f\"Choice {i}\"]\n",
    "            if isinstance(entry, dict):\n",
    "                reasoning = entry.get(\"reasoning\", \"\")\n",
    "                score = float(entry.get(\"score\", 0.0))\n",
    "            else:\n",
    "                # dict 아닌 경우 숫자로 변환\n",
    "                reasoning = \"\"\n",
    "                score = float(entry)\n",
    "        else:\n",
    "            # case 2: 단순 문자열에서 점수 추출\n",
    "            reasoning = \"\"\n",
    "            match = re.search(f\"Choice {i}.*?([0-9]*\\\\.?[0-9]+)\", str(output_json))\n",
    "            score = float(match.group(1)) if match else 0.0\n",
    "\n",
    "        scores.append(score)\n",
    "        print(f\"--- Choice {i} ---\")\n",
    "        print(f\"Reasoning: {reasoning}\")\n",
    "        print(f\"Score: {score}\\n\")\n",
    "\n",
    "    recommended_choice = scores.index(max(scores))\n",
    "    print(f\"Recommended Choice: {recommended_choice} (score {max(scores):.4f})\\n\")\n",
    "    \n",
    "    \n",
    "# 4. 사용 예시\n",
    "# =========================\n",
    "evaluate_piqa_sample_llm(2)   # train셋 2번 샘플 평가\n",
    "evaluate_piqa_sample_llm(5, split=\"validation\")  # validation셋 5번 샘플 평가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba68ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1️⃣ XNLI 점수 계산 및 저장\n",
    "# =========================\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 데이터셋\n",
    "dataset = load_dataset(\"piqa\", trust_remote_code=True)\n",
    "\n",
    "# XNLI 모델\n",
    "xnli_model_name = \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n",
    "xnli_tokenizer = AutoTokenizer.from_pretrained(xnli_model_name)\n",
    "xnli_model = AutoModelForSequenceClassification.from_pretrained(xnli_model_name)\n",
    "labels = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "\n",
    "# XNLI 점수 계산 함수\n",
    "def get_xnli_scores(premise, hypothesis):\n",
    "    inputs = xnli_tokenizer(premise, hypothesis, return_tensors=\"pt\", truncation=True)\n",
    "    with torch.no_grad():\n",
    "        logits = xnli_model(**inputs).logits\n",
    "    probs = F.softmax(logits, dim=-1)[0].tolist()\n",
    "    return {\n",
    "        \"entailment\": probs[0],\n",
    "        \"neutral\": probs[1],\n",
    "        \"contradiction\": probs[2],\n",
    "        \"ec_score\": probs[0] - probs[2]\n",
    "    }\n",
    "\n",
    "# 데이터셋별 XNLI 점수 계산 및 저장\n",
    "def save_xnli_scores(split=\"train\", output_file=\"piqa_train_xnli.json\"):\n",
    "    results = []\n",
    "    for idx in tqdm(range(len(dataset[split]))):\n",
    "        sample = dataset[split][idx]\n",
    "        goal = sample['goal']\n",
    "        choices = [sample['sol1'], sample['sol2']]\n",
    "        answer = sample['label']\n",
    "\n",
    "        scores = [get_xnli_scores(goal, c) for c in choices]\n",
    "\n",
    "        result = {\n",
    "            \"index\": idx,\n",
    "            \"goal\": goal,\n",
    "            \"choices\": choices,\n",
    "            \"answer\": answer,\n",
    "            \"xnli_scores\": scores\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(results)} samples to {output_file}\")\n",
    "\n",
    "# 예시 실행\n",
    "save_xnli_scores(split=\"train\", output_file=\"piqa_train_xnli.json\")\n",
    "save_xnli_scores(split=\"validation\", output_file=\"piqa_validation_xnli.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero(hy-eval)",
   "language": "python",
   "name": "hy-eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
