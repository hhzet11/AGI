{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe3bb0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apitrain2/miniconda3/envs/hyenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-09 08:07:34.156960: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-09 08:07:34.177224: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757405254.197310 3249787 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757405254.203679 3249787 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1757405254.219798 3249787 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757405254.219815 3249787 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757405254.219817 3249787 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757405254.219819 3249787 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-09 08:07:34.224362: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train에서 중복 제거: 10203개 삭제 (최종 195224개)\n",
      "dev에서 중복 제거: 205개 삭제 (최종 5031개)\n",
      "test에서 중복 제거: 1187개 삭제 (최종 22796개)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from bert_score import score as bert_score\n",
    "from konlpy.tag import Okt\n",
    "import sacrebleu\n",
    "\n",
    "\n",
    "train = pd.read_csv('/home/apitrain2/hy_agi/phyEn2Ko/0902.tsv', sep='\\t', header=0, names=['input', 'relation', 'output', 'sentence', 'vera_score', 'input_ko', 'output_ko', 'relation_ko'])\n",
    "dev = pd.read_csv('/home/apitrain2/hy_agi/phyEn2Ko/dev_ko.tsv', sep='\\t', header=0, names=['input', 'relation', 'output', 'sentence', 'vera_score', 'input_ko', 'output_ko', 'relation_ko'])\n",
    "test = pd.read_csv('/home/apitrain2/hy_agi/phyEn2Ko/test_ko.tsv', sep='\\t', header=0, names=['input', 'relation', 'output', 'input_ko', 'output_ko', 'relation_ko']) \n",
    "\n",
    "train = train[[\"input_ko\", \"relation_ko\", \"output_ko\"]]\n",
    "dev = dev[[\"input_ko\", \"relation_ko\", \"output_ko\"]]\n",
    "test = test[[\"input_ko\", \"relation_ko\", \"output_ko\"]]\n",
    "\n",
    "def drop_duplicates_and_report(df, name):\n",
    "    before = len(df)\n",
    "    df_dedup = df.drop_duplicates(subset=[\"input_ko\", \"relation_ko\", \"output_ko\"])\n",
    "    after = len(df_dedup)\n",
    "    print(f\"{name}에서 중복 제거: {before - after}개 삭제 (최종 {after}개)\")\n",
    "    return df_dedup\n",
    "\n",
    "train = drop_duplicates_and_report(train, \"train\")\n",
    "dev = drop_duplicates_and_report(dev, \"dev\")\n",
    "test = drop_duplicates_and_report(test, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4ef4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(151665, 1536)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "login(\"\") # huggingface token\n",
    "\n",
    "# ---------- 모델/토크나이저 로드 ----------\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\" #\"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 모델을 로드할 때는 low_cpu_mem_usage=True 권장\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=False\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ba97e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def make_messages(head, relation, tail=None):\n",
    "    \"\"\"\n",
    "    tail=None이면 predict용, tail=str이면 학습용\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"당신은 개체와 관련된 정보를 정확히 생성하는 유용한 어시스턴트입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"'{head}'의 '{relation}'에 해당하는 결과를 한국어 명사구로 알려줘.\"}\n",
    "    ]\n",
    "    # 학습용일 때만 assistant content 추가\n",
    "    if tail is not None:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": tail})\n",
    "    return messages\n",
    "\n",
    "class HeadRelationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=64):\n",
    "        \"\"\"\n",
    "        data: pandas DataFrame with columns ['input_ko', 'relation_ko', 'output_ko']\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        head, relation, tail = row['input_ko'], row['relation_ko'], row['output_ko']\n",
    "\n",
    "        messages = make_messages(head, relation, tail)  # tail 포함\n",
    "\n",
    "        # prompt 문자열 (assistant 제외)\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            messages[:-1],  # system + user\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        input_ids = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids[0]\n",
    "\n",
    "        # labels: assistant content만 학습\n",
    "        labels = self.tokenizer(\n",
    "            messages[-1]['content'],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids[0]\n",
    "\n",
    "        # prompt 부분은 학습 제외\n",
    "        labels[:len(input_ids)] = -100\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "train_dataset = HeadRelationDataset(train.iloc[:1000], tokenizer)\n",
    "dev_dataset = HeadRelationDataset(dev.iloc[:100], tokenizer)\n",
    "test_dataset = HeadRelationDataset(test, tokenizer)\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "596f887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from bert_score import score as bert_score\n",
    "from konlpy.tag import Okt\n",
    "import sacrebleu\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    BLEU-1~4, sacreBLEU, chrF, chrF++, CIDEr, BERTScore, eval_loss 계산\n",
    "    \"\"\"\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "\n",
    "        # tensor 변환\n",
    "        predictions = torch.tensor(predictions, dtype=torch.float32) if not isinstance(predictions, torch.Tensor) else predictions\n",
    "        labels = torch.tensor(labels, dtype=torch.long) if not isinstance(labels, torch.Tensor) else labels\n",
    "\n",
    "        # Shift for causal LM\n",
    "        shift_logits = predictions[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "        shift_labels = shift_labels.view(-1)\n",
    "        valid_mask = shift_labels != -100\n",
    "        valid_logits = shift_logits[valid_mask]\n",
    "        valid_labels = shift_labels[valid_mask]\n",
    "\n",
    "        if len(valid_labels) == 0:\n",
    "            return {\n",
    "                \"bleu1\": 0.0, \"bleu2\": 0.0, \"bleu3\": 0.0, \"bleu4\": 0.0,\n",
    "                \"sacrebleu\": 0.0, \"chrf\": 0.0, \"chrfpp\": 0.0,\n",
    "                \"cider\": 0.0, \"bertscore\": 0.0, \"eval_loss\": float(\"inf\")\n",
    "            }\n",
    "\n",
    "        # eval_loss\n",
    "        loss = F.cross_entropy(valid_logits, valid_labels)\n",
    "        eval_loss = loss.item()\n",
    "\n",
    "        # === Generation for NLG metrics ===\n",
    "        from transformers import AutoTokenizer\n",
    "        global tokenizer\n",
    "        if tokenizer is None:\n",
    "            raise ValueError(\"tokenizer must be defined globally for compute_metrics\")\n",
    "\n",
    "        pred_token_ids = torch.argmax(predictions, dim=-1)\n",
    "        label_token_ids = labels\n",
    "\n",
    "        pred_texts, label_texts = [], []\n",
    "        for pred_ids, label_ids in zip(pred_token_ids, label_token_ids):\n",
    "            label_ids = [i for i in label_ids.tolist() if i != -100 and i != tokenizer.pad_token_id]\n",
    "            pred_ids = [i for i in pred_ids.tolist() if i != -100 and i != tokenizer.pad_token_id]\n",
    "            pred_texts.append(tokenizer.decode(pred_ids, skip_special_tokens=True).strip())\n",
    "            label_texts.append(tokenizer.decode(label_ids, skip_special_tokens=True).strip())\n",
    "\n",
    "        # BLEU-1~4\n",
    "        bleu1s, bleu2s, bleu3s, bleu4s = [], [], [], []\n",
    "        smoothie = SmoothingFunction().method1\n",
    "        for ref, hyp in zip(label_texts, pred_texts):\n",
    "            ref_tokens = [ref.split()]\n",
    "            hyp_tokens = hyp.split()\n",
    "            bleu1s.append(sentence_bleu(ref_tokens, hyp_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie))\n",
    "            bleu2s.append(sentence_bleu(ref_tokens, hyp_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie))\n",
    "            bleu3s.append(sentence_bleu(ref_tokens, hyp_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie))\n",
    "            bleu4s.append(sentence_bleu(ref_tokens, hyp_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie))\n",
    "\n",
    "        # sacreBLEU (sentence-level)\n",
    "        sacrebleu_scores = []\n",
    "        for ref, hyp in zip(label_texts, pred_texts):\n",
    "            try:\n",
    "                hyp_tok = ' '.join(okt.morphs(hyp))\n",
    "                ref_tok = ' '.join(okt.morphs(ref))\n",
    "                bleu_score = sacrebleu.sentence_bleu(hyp_tok, [ref_tok])\n",
    "                sacrebleu_scores.append(bleu_score.score)\n",
    "            except:\n",
    "                sacrebleu_scores.append(0.0)\n",
    "\n",
    "        # chrF / chrF++\n",
    "        try:\n",
    "            chrf_score = sacrebleu.corpus_chrf(pred_texts, [label_texts], beta=2.0).score\n",
    "            chrfpp_score = sacrebleu.corpus_chrf(pred_texts, [label_texts], beta=2.0, word_order=2).score\n",
    "        except:\n",
    "            chrf_score = 0.0\n",
    "            chrfpp_score = 0.0\n",
    "\n",
    "        # CIDEr\n",
    "        try:\n",
    "            from pycocoevalcap.cider.cider import Cider\n",
    "            gts = {i: [label_texts[i]] for i in range(len(label_texts))}\n",
    "            res = {i: [pred_texts[i]] for i in range(len(pred_texts))}\n",
    "            cider_scorer = Cider()\n",
    "            cider_score, _ = cider_scorer.compute_score(gts, res)\n",
    "        except:\n",
    "            cider_score = 0.0\n",
    "\n",
    "        # BERTScore\n",
    "        try:\n",
    "            P, R, F1 = bert_score(pred_texts, label_texts, lang=\"ko\", verbose=False)\n",
    "            bertscore = float(F1.mean())\n",
    "        except:\n",
    "            bertscore = 0.0\n",
    "\n",
    "        return {\n",
    "            \"bleu1\": float(np.mean(bleu1s)),\n",
    "            \"bleu2\": float(np.mean(bleu2s)),\n",
    "            \"bleu3\": float(np.mean(bleu3s)),\n",
    "            \"bleu4\": float(np.mean(bleu4s)),\n",
    "            \"sacrebleu\": float(np.mean(sacrebleu_scores)),\n",
    "            \"chrf\": float(chrf_score),\n",
    "            \"chrfpp\": float(chrfpp_score),\n",
    "            \"cider\": float(cider_score),\n",
    "            \"bertscore\": float(bertscore),\n",
    "            \"eval_loss\": eval_loss,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_metrics: {e}\")\n",
    "        return {\n",
    "            \"bleu1\": 0.0, \"bleu2\": 0.0, \"bleu3\": 0.0, \"bleu4\": 0.0,\n",
    "            \"sacrebleu\": 0.0, \"chrf\": 0.0, \"chrfpp\": 0.0,\n",
    "            \"cider\": 0.0, \"bertscore\": 0.0, \"eval_loss\": float(\"inf\")\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a4777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3249787/3953113804.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-09 08:07:46,182] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apitrain2/miniconda3/envs/hyenv/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/apitrain2/miniconda3/envs/hyenv/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-09 08:07:47,848] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='167' max='167' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [167/167 01:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu1</th>\n",
       "      <th>Bleu2</th>\n",
       "      <th>Bleu3</th>\n",
       "      <th>Bleu4</th>\n",
       "      <th>Sacrebleu</th>\n",
       "      <th>Chrf</th>\n",
       "      <th>Chrfpp</th>\n",
       "      <th>Cider</th>\n",
       "      <th>Bertscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>0.394956</td>\n",
       "      <td>0.810119</td>\n",
       "      <td>0.723854</td>\n",
       "      <td>0.672577</td>\n",
       "      <td>0.620261</td>\n",
       "      <td>73.130777</td>\n",
       "      <td>82.390199</td>\n",
       "      <td>82.246909</td>\n",
       "      <td>0.031777</td>\n",
       "      <td>0.947924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apitrain2/miniconda3/envs/hyenv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=167, training_loss=0.05355669495588291, metrics={'train_runtime': 104.2228, 'train_samples_per_second': 9.595, 'train_steps_per_second': 1.602, 'total_flos': 503170793472000.0, 'train_loss': 0.05355669495588291, 'epoch': 1.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    learning_rate = 5e-5,\n",
    "    weight_decay = 0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16 = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf97e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/5 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Predicting: 100%|██████████| 5/5 [00:18<00:00,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD: 은행계좌, RELATION: 개체용도, PREDICT: '개체용도'의 '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로\n",
      "HEAD: 은행계좌, RELATION: 개체용도, PREDICT: '개체용도'의 '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로\n",
      "HEAD: 은행계좌, RELATION: 개체용도, PREDICT: '개체용도'의 '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로\n",
      "HEAD: 은행계좌, RELATION: 개체용도, PREDICT: '개체용도'의 '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로\n",
      "HEAD: 은행계좌, RELATION: 개체용도, PREDICT: '개체용도'의 '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로\n",
      "HEAD: 은행계좌, RELATION: 개체용도, PREDICT: '개체용도'의 '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로\n",
      "HEAD: 은행계좌, RELATION: 개체용도, PREDICT: '개체용도'의 '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로\n",
      "HEAD: 은행계좌, RELATION: 개체용도, PREDICT: '개체용도'의 '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로\n",
      "HEAD: 은행계좌, RELATION: 개체용도, PREDICT: '개체용도'의 '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로\n",
      "HEAD: 은행계좌, RELATION: 개체용도, PREDICT: '개체용도'의 '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로 알려줘. '개체용도'에 해당하는 결과를 한국어 명사구로\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# Prompt 생성\n",
    "# -----------------------------\n",
    "def make_prompt_str(head, relation):\n",
    "    system = \"당신은 개체와 관련된 정보를 정확히 생성하는 유용한 어시스턴트입니다.\"\n",
    "    user = f\"'{head}'의 '{relation}'에 해당하는 결과를 한국어 명사구로 알려줘.\"\n",
    "    # tail 생성 유도\n",
    "    prompt = f\"{system}\\n{user}\\nAssistant:\"\n",
    "    return prompt\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset 정의\n",
    "# -----------------------------\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return row['input_ko'], row['relation_ko']\n",
    "\n",
    "# -----------------------------\n",
    "# Batch predict 함수\n",
    "# -----------------------------\n",
    "def predict_batch(model, tokenizer, test_df, batch_size=4, max_new_tokens=64):\n",
    "    dataset = TestDataset(test_df)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "        heads, relations = batch\n",
    "        for head, relation in zip(heads, relations):\n",
    "            prompt = make_prompt_str(head, relation)\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=max_new_tokens,  # tail 길이\n",
    "                    do_sample=False,               # greedy\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            # Assistant: 뒤의 tail만 추출\n",
    "            if \"Assistant:\" in output_text:\n",
    "                output_text = output_text.split(\"Assistant:\")[-1].strip()\n",
    "\n",
    "            results.append(output_text)\n",
    "\n",
    "            # GPU 메모리 정리\n",
    "            del input_ids, output_ids\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return results\n",
    "\n",
    "# -----------------------------\n",
    "# Predict 실행\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# test 데이터 일부만 예시\n",
    "predictions = predict_batch(model, tokenizer, test.iloc[:10], batch_size=2, max_new_tokens=64)\n",
    "\n",
    "for i, row in enumerate(test.iloc[:10].itertuples()):\n",
    "    print(f\"HEAD: {row.input_ko}, RELATION: {row.relation_ko}, PREDICT: {predictions[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed50d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
